{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Search + NER Demo\n",
    "\n",
    "Built following the excellent [blog post by Camille Corti-Georgiou](https://www.elastic.co/search-labs/blog/articles/developing-an-elastic-search-app-with-streamlit-semantic-search-and-named-entity-extraction).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import getpass\n",
    "import os\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from getpass import getpass\n",
    "\n",
    "from elasticsearch import Elasticsearch, exceptions\n",
    "from elasticsearch.helpers import parallel_bulk\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "from tqdm.auto import tqdm  # auto selects notebook GUI if in Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset from Kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the Kaggle API\n",
    "os.environ[\"KAGGLE_USERNAME\"] = getpass.getpass(\"Enter your Kaggle username:\")\n",
    "os.environ[\"KAGGLE_KEY\"] = getpass.getpass(\"Enter your Kaggle key:\")\n",
    "\n",
    "# Initialize API\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# Define the dataset path\n",
    "dataset = \"gpreda/bbc-news\"\n",
    "\n",
    "# Define download path\n",
    "download_path = \"../data\"  # Adjust this path as needed\n",
    "\n",
    "# Download dataset\n",
    "api.dataset_download_files(dataset, path=download_path, unzip=True)\n",
    "\n",
    "print(f\"Dataset downloaded to: {download_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Docker to Upload and Start the Custom NER ML Model\n",
    "\n",
    "```sh\n",
    "docker run -it --rm docker.elastic.co/eland/eland:latest \\\n",
    "    eland_import_hub_model \\\n",
    "        --cloud-id $CLOUD_ID \\\n",
    "        --es-api-key $API_KEY \\\n",
    "        --hub-model-id \"elastic/distilbert-base-uncased-finetuned-conll03-english\" \\\n",
    "        --task-type ner \\\n",
    "        --start\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Elasticsearch Client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the client instance\n",
    "es_client = Elasticsearch(\n",
    "    # For local development\n",
    "    # hosts=[\"http://localhost:9200\"]\n",
    "    cloud_id=getpass(\"Elastic Cloud ID: \"),\n",
    "    api_key=getpass(\"Elastic Api Key: \"),\n",
    ")\n",
    "\n",
    "print(es_client.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Deploy ELSER Model\n",
    "\n",
    "### Helper functions to delete the model check model status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_existing_model(es_client: str, model_id: str) -> bool:\n",
    "    \"\"\"\n",
    "    Deletes an existing machine learning model in Elasticsearch.\n",
    "\n",
    "    This function attempts to delete a specified machine learning model by its ID. It will return True if the model\n",
    "    is successfully deleted or if the deletion is acknowledged by Elasticsearch. If the model does not exist or if\n",
    "    an error occurs during deletion, it catches the NotFoundError exception and returns False.\n",
    "\n",
    "    Args:\n",
    "        es_client (str): The Elasticsearch client instance used to connect to and interact with Elasticsearch.\n",
    "        model_id (str): The unique identifier for the machine learning model to be deleted.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the model was successfully deleted or does not exist, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Attempt to delete the model and check if the deletion is acknowledged\n",
    "        response = es_client.ml.delete_trained_model(model_id=model_id, force=True)\n",
    "        if response.get(\"acknowledged\"):\n",
    "            # success!\n",
    "            print(\"Model deleted successfully, we will proceed with creating one\")\n",
    "\n",
    "            return True\n",
    "\n",
    "        else:\n",
    "            print(f\"There was an issue deleting the model: {response}\")\n",
    "            return False\n",
    "\n",
    "    except exceptions.NotFoundError:\n",
    "        # Model does not exist, so return True indicates you can proceed\n",
    "        print(\"Model doesn't exist, but we will proceed with creating one\")\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "# Function to check if the model is downloaded and ready for deployment\n",
    "def check_model_download_status(es_client: str, model_id: str) -> bool:\n",
    "    while True:\n",
    "        status = es_client.ml.get_trained_models(\n",
    "            model_id=model_id, include=\"definition_status\"\n",
    "        )\n",
    "\n",
    "        # Check if the model is fully defined and ready to be deployed\n",
    "        if status[\"trained_model_configs\"][0][\"fully_defined\"]:\n",
    "            print(\"ELSER Model is downloaded and ready to be deployed.\")\n",
    "\n",
    "            return True\n",
    "        else:\n",
    "            print(\"ELSER Model is downloaded but not ready to be deployed.\")\n",
    "        # Wait for 5 seconds before checking the status again\n",
    "        time.sleep(5)\n",
    "\n",
    "\n",
    "def check_model_deployment_status(es_client: str, model_id: str) -> bool:\n",
    "    while True:\n",
    "        status = es_client.ml.get_trained_models_stats(\n",
    "            model_id=model_id,\n",
    "        )\n",
    "\n",
    "        if status[\"trained_model_stats\"][0][\"deployment_stats\"][\"state\"] == \"started\":\n",
    "            print(\"ELSER Model has been successfully deployed.\")\n",
    "\n",
    "            return True\n",
    "\n",
    "        else:\n",
    "            print(\"ELSER Model is currently being deployed.\")\n",
    "\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the ELSER model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ID for the model we're working with\n",
    "model_id = \".elser_model_2_linux-x86_64\"\n",
    "\n",
    "# Call the function to delete the model if it exists\n",
    "if delete_existing_model(es_client, model_id):\n",
    "    # Create the ELSER model configuration and automatically download the model if it doesn't exist\n",
    "    es_client.ml.put_trained_model(\n",
    "        model_id=model_id, input={\"field_names\": [\"text_field\"]}\n",
    "    )\n",
    "\n",
    "    # Call the function to check the model status\n",
    "    check_model_download_status(es_client, model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is downloaded, we can deploy the model in our ML node. Use the following command to deploy the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start trained model deployment if not already deployed\n",
    "try:\n",
    "    es_client.ml.start_trained_model_deployment(\n",
    "        model_id=model_id,\n",
    "        wait_for=\"starting\",\n",
    "        queue_capacity=10000,\n",
    "        number_of_allocations=1,\n",
    "        threads_per_allocation=2,\n",
    "    )\n",
    "\n",
    "except exceptions.BadRequestError as e:\n",
    "    print(e)\n",
    "\n",
    "# Call the function to check the model status\n",
    "check_model_deployment_status(es_client, model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Elasticsearch Index & Ingest Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Ingest Pipeline processors, and index mappings\n",
    "\n",
    "First we need to define the json representing the ingest pipeline processors, and the index mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_news_pipeline_processors = [\n",
    "    {\"date\": {\"field\": \"pubDate\", \"formats\": [\"EEE, dd MMM yyyy HH:mm:ss zzz\"]}},\n",
    "    {\n",
    "        \"inference\": {\n",
    "            \"model_id\": \".elser_model_2\",\n",
    "            \"input_output\": [\n",
    "                {\"input_field\": \"title\", \"output_field\": \"ml-elser-title\"}\n",
    "            ],\n",
    "            \"description\": 'Runs .elser_model_2 and stores resulting tokens in \"ml-elser-title\"',\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"inference\": {\n",
    "            \"model_id\": \".elser_model_2\",\n",
    "            \"input_output\": [\n",
    "                {\"input_field\": \"description\", \"output_field\": \"ml-elser-description\"}\n",
    "            ],\n",
    "            \"description\": 'Runs .elser_model_2 and stores resulting tokens in \"ml-elser-description\"',\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"inference\": {\n",
    "            \"model_id\": \"elastic__distilbert-base-uncased-finetuned-conll03-english\",\n",
    "            \"target_field\": \"ml.ner\",\n",
    "            \"field_map\": {\"title\": \"text_field\"},\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"script\": {\n",
    "            \"lang\": \"painless\",\n",
    "            \"if\": \"return ctx['ml']['ner'].containsKey('entities')\",\n",
    "            \"source\": \"Map tags = new HashMap(); for (item in ctx['ml']['ner']['entities']) { if (!tags.containsKey(item.class_name)) tags[item.class_name] = new HashSet(); tags[item.class_name].add(item.entity);} ctx['tags'] = tags;\",\n",
    "        }\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_news_index_mapping = {\n",
    "    \"_source\": {\"excludes\": [\"content_embedding\"]},\n",
    "    \"properties\": {\n",
    "        \"@timestamp\": {\"type\": \"date\"},\n",
    "        \"@version\": {\"type\": \"keyword\", \"ignore_above\": 256},\n",
    "        \"description\": {\"type\": \"text\"},\n",
    "        \"ml\": {\n",
    "            \"properties\": {\n",
    "                \"ner\": {\n",
    "                    \"properties\": {\n",
    "                        \"entities\": {\n",
    "                            \"properties\": {\n",
    "                                \"class_name\": {\"type\": \"keyword\", \"ignore_above\": 256},\n",
    "                                \"class_probability\": {\"type\": \"float\"},\n",
    "                                \"end_pos\": {\"type\": \"long\"},\n",
    "                                \"entity\": {\"type\": \"keyword\", \"ignore_above\": 256},\n",
    "                                \"start_pos\": {\"type\": \"long\"},\n",
    "                            }\n",
    "                        },\n",
    "                        \"model_id\": {\"type\": \"keyword\", \"ignore_above\": 256},\n",
    "                        \"predicted_value\": {\"type\": \"keyword\", \"ignore_above\": 256},\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"ml-elser-description\": {\"type\": \"sparse_vector\"},\n",
    "        \"ml-elser-title\": {\"type\": \"sparse_vector\"},\n",
    "        \"pubDate\": {\n",
    "            \"type\": \"date\",\n",
    "            \"format\": \"EEE, dd MMM yyyy HH:mm:ss z\",\n",
    "            \"ignore_malformed\": True,\n",
    "        },\n",
    "        \"tags\": {\n",
    "            \"properties\": {\n",
    "                \"LOC\": {\"type\": \"keyword\", \"ignore_above\": 256},\n",
    "                \"MISC\": {\"type\": \"keyword\", \"ignore_above\": 256},\n",
    "                \"ORG\": {\"type\": \"keyword\", \"ignore_above\": 256},\n",
    "                \"PER\": {\"type\": \"keyword\", \"ignore_above\": 256},\n",
    "            }\n",
    "        },\n",
    "        \"title\": {\"type\": \"text\"},\n",
    "        \"url\": {\"type\": \"keyword\", \"ignore_above\": 256},\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, to create the ingest pipeline and search index...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_client.ingest.put_pipeline(\n",
    "    id=\"search-demos-bbc.semantic.ner-pipeline\",\n",
    "    description=\"Ingest pipeline for Semantic Ner Search Demo using BBC dataset\",\n",
    "    processors=es_news_pipeline_processors,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_index = \"search-demos-bbc.semantic.ner\"\n",
    "\n",
    "es_client.indices.delete(index=\"search-demos-bbc.semantic.ner\", ignore_unavailable=True)\n",
    "es_client.indices.create(\n",
    "    index=es_index,\n",
    "    settings={\"index\": {\"default_pipeline\": \"search-demos-bbc.semantic.ner-pipeline\"}},\n",
    "    mappings=es_news_index_mapping,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the CSV data and index into Elasticsearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your dataset file\n",
    "dataset_path = \"../data/bbc_news.csv\"\n",
    "\n",
    "# Read the CSV file into a dict\n",
    "data = []\n",
    "with open(dataset_path, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    # Create a DictReader object\n",
    "    csv_reader = csv.DictReader(file)\n",
    "\n",
    "    # Iterate over the rows in the CSV file\n",
    "    for row in csv_reader:\n",
    "        data.append(row)\n",
    "\n",
    "# Now 'data' is a list of dictionaries, where each dictionary represents a row from the CSV file\n",
    "print(data[0:5])  # Print the first row to verify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some helper functions for a smooth ingest experience\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_actions(documents: list):\n",
    "    \"\"\"\n",
    "    Generates actions for bulk ingest from a list of dictionaries.\n",
    "\n",
    "    Args:\n",
    "        documents (list): A list of dictionaries, where each dictionary is a document.\n",
    "\n",
    "    Yields:\n",
    "        dict: A dictionary representing an action for bulk ingest.\n",
    "    \"\"\"\n",
    "    for document in documents:\n",
    "        yield document\n",
    "\n",
    "\n",
    "def send_to_elasticsearch(\n",
    "    es_client: Elasticsearch,\n",
    "    es_index: str,\n",
    "    documents: list,\n",
    "    chunk_size: int = 500,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Uploads data to an Elasticsearch index using parallel processing.\n",
    "\n",
    "    Args:\n",
    "        es_client (Elasticsearch): The Elasticsearch client.\n",
    "        es_index (str): The name of the Elasticsearch index.\n",
    "        documents (list): A list of dictionaries to index.\n",
    "        chunk_size (int): The number of documents to send in each bulk request.\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating the number of successfully indexed documents.\n",
    "    \"\"\"\n",
    "    num_documents = len(documents)\n",
    "    progress = tqdm(unit=\"docs\", total=num_documents)\n",
    "    successes = 0\n",
    "\n",
    "    try:\n",
    "        for ok, info in parallel_bulk(\n",
    "            client=es_client,\n",
    "            index=es_index,\n",
    "            actions=generate_actions(documents),\n",
    "            chunk_size=chunk_size,\n",
    "            request_timeout=360,\n",
    "        ):\n",
    "            progress.update(1)\n",
    "            progress.refresh()\n",
    "\n",
    "            if ok:\n",
    "                successes += 1\n",
    "            else:\n",
    "                print(f\"A document failed to index: {info['index']['error']}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An Elasticsearch error occurred:\", e)\n",
    "        if hasattr(e, \"errors\"):\n",
    "            for error_detail in e.errors:\n",
    "                print(error_detail)\n",
    "\n",
    "    return f\"Indexed {successes}/{num_documents} documents.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual data ingest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 8\n",
    "thread_count = 16\n",
    "chunk_size = 1000\n",
    "batch_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_in_batches(es_client, es_index, documents, batch_size=500):\n",
    "    total_docs = len(documents)\n",
    "    start_index = 0\n",
    "\n",
    "    while start_index < total_docs:\n",
    "        # Calculate the end index for the current batch, ensuring it doesn't exceed total_docs\n",
    "        end_index = min(start_index + batch_size, total_docs)\n",
    "        current_batch = documents[start_index:end_index]\n",
    "\n",
    "        # Upload the current batch\n",
    "        result = send_to_elasticsearch(\n",
    "            es_client=es_client,\n",
    "            es_index=es_index,\n",
    "            documents=current_batch,\n",
    "            chunk_size=batch_size,  # This ensures parallel_bulk processes in chunks of 500 as well\n",
    "        )\n",
    "\n",
    "        # Optionally, print/log the result of each batch upload\n",
    "        print(result)\n",
    "\n",
    "        # Update the start index for the next batch\n",
    "        start_index += batch_size\n",
    "\n",
    "\n",
    "# Use the function\n",
    "upload_in_batches(es_client, es_index, data, batch_size=batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "922c6dc2f997cff1ad2bba8520fc1b4702789a70b32d82613e8aa6224a20cd8e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
